\section{LLM-Driven Interaction Technologies}
\label{sec:llmtech}

A more interesting section for me as I work at Sagel AI regarding realtime telephony agents that can take agentic actions.
I will give a general overview of the general Multimodal LLM Interaction Technologies.

\subsection{Text-Based Interfaces, Planning, and Tool Use}
This is plain text interfaces similar to ChatGPT or LLaMA or DeepSeek.

\subsection{Audio Pipelines: Speech-to-Text $\rightarrow$ LLM $\rightarrow$ Text-to-Speech}
This is how speech-to-speech interfaces were being done a few years ago by translating input speech into text, feeding it into an LLM, and then turning the resulting output into speech again.
It is a solid approach, however quite slow. Easier to integrate with RAG though.

\subsection{Speech-to-Speech LLMs}
This is the state of the art regarding speech-to-speech man-machine interactions ever since the launch of OpenAI Realtime models.
It works basically by taking input speech in divided chunks and feeding them into the model as they come.
The model starts outputting directly after the first few chunks of audio come in, giving it a near realtime feel during conversation.
Very fast, however cognitively a lot dumber than the top-of-the-line models such as GPT-4o or GPT-5.
Quite hard to integrate with RAG but not impossible. There are a few papers I saw regarding this but never done it myself; currently working on this at work.

\subsection{Vision-Language(-Action) Models}
I have learned of this sort of model family not so long ago, and while looking for papers for this seminar I came across these again.
Basically this model family can draw context from visual streams such as video feeds and take action.
I need to read more on this to give a better explanation but certainly very interesting in context of this seminar topic.
